{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided not to use this notebook due to the prohibitive time complexity of run_finbert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_text(text, max_length=510):\n",
    "    # Split the text into words\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = ''\n",
    "        while i < len(words) and len(tokenizer.tokenize(chunk + ' ' + words[i])) <= max_length:\n",
    "            chunk += ' ' + words[i]\n",
    "            i += 1\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def get_transcripts(filepath, transcript_column=\"transcript\"):\n",
    "    \"\"\"Returns a list consisting of all transcripts at given filepath.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    transcripts = df[transcript_column].tolist()\n",
    "    return transcripts\n",
    "\n",
    "def run_finbert(transcripts):\n",
    "    final_scores = []\n",
    "    i = 0\n",
    "    length = len(transcripts)\n",
    "\n",
    "    for transcript in transcripts:\n",
    "        sliding_chunks = sliding_window_text(transcript)\n",
    "\n",
    "        scores = [0, 0, 0]  # [negativity, neutrality, positivity]\n",
    "\n",
    "        for chunk in sliding_chunks:\n",
    "            # Encode the text snippet and convert to torch tensor\n",
    "            inputs = tokenizer.encode_plus(chunk, return_tensors='pt')\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # The sentiment scores are the elements of the first output\n",
    "            sentiment_scores = outputs[0][0].detach().numpy()\n",
    "\n",
    "        # Add the sentiment scores to the cumulative scores\n",
    "        scores = [sum(x) for x in zip(scores, sentiment_scores)]\n",
    "        \n",
    "        # Calculate synthetic sentiment score\n",
    "        synthetic_score = (scores[0] - scores[2]) / (scores[0] + scores[2] + 1)\n",
    "        \n",
    "        final_scores.append(synthetic_score)\n",
    "        if i%10 == 0:\n",
    "            print(f\"Done: {i/length}\")\n",
    "            save_intermediate_results(final_scores, 'FINBERT-intermediate_results.csv')\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return final_scores\n",
    "\n",
    "def save_intermediate_results(scores, filename):\n",
    "    df = pd.DataFrame({'sentiment_score': scores})\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "sentiment_scores = run_finbert(get_transcripts(\"..\\data\\\\final_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize sentiment scores to be between -1 and 1:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "sentiment_scores = np.array(sentiment_scores)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "normalized_scores = scaler.fit_transform(sentiment_scores.reshape(-1, 1))\n",
    "\n",
    "normalized_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add normalized_scores to final_dataset.csv: (We never got this far).\n",
    "\"\"\"\n",
    "\n",
    "whole_dataset = pd.read_csv(\"..\\data\\\\final_dataset.csv\")\n",
    "\n",
    "whole_dataset['finbert'] = normalized_scores\n",
    "\n",
    "whole_dataset.to_csv(\"..\\data\\\\final_dataset.csv\", index=False)\n",
    "\n",
    "\n",
    "no_transcripts = pd.read_csv(\"..\\data\\\\no_transcript_final_dataset.csv\")\n",
    "whole_dataset.drop(columns=\"transcript\", inplace=True)\n",
    "# df\n",
    "no_transcripts.to_csv(\"..\\data\\\\no_transcript_final_dataset.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
