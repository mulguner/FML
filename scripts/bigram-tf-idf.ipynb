{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "In this document, we assign sentiment scores to each document following the procedure developed in Garcia et al. (2022).\n",
    "\n",
    "To process TF-IDFs, we should really go one by one as a new transcript is released, continuously updating the entire dtm matrix. \n",
    "To limit time complexity, we instead run the vectorization once, on the whole dataset. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_transcripts(docs, include_unigrams=False, lm=False, ghr=False):\n",
    "    \"\"\"\n",
    "    Docs is a list of transcripts.\\n\n",
    "    Returns bigram document-term-matrices, one for the positive dictionary, one for the negative.  \n",
    "    \"\"\"\n",
    "    with open(\"..\\data\\\\bigrams_dict.txt\") as file: # Get bigrams from txt file \n",
    "        string = file.read()\n",
    "\n",
    "    posneg = string.split(\"#\")\n",
    "    positive_voc = posneg[0].split(\", \")\n",
    "    negative_voc = posneg[1].split(\", \")\n",
    "\n",
    "    sws = list(set(stopwords.words('english'))) # We use this stopword dictionary, as Scikit warns that its stopword list is unreliable \n",
    "\n",
    "    if include_unigrams:\n",
    "        if lm: # Use Loughran-Mcdonald unigrams\n",
    "            lm_dict = pd.read_csv(\"..\\data\\Loughran-McDonald_MasterDictionary_1993-2023.csv\")\n",
    "            positive_LM_tokens = [token.lower() for token in lm_dict[lm_dict[\"Positive\"] > 0][\"Word\"].tolist()]\n",
    "            negative_LM_tokens = [token.lower() for token in lm_dict[lm_dict[\"Negative\"] > 0][\"Word\"].tolist()]\n",
    "            positive_voc.extend(positive_LM_tokens)\n",
    "            negative_voc.extend(negative_LM_tokens)\n",
    "\n",
    "        elif ghr: # Use Garcia, Hu, Roher (2022) unigrams\n",
    "            positive_voc.extend([ \"above\", \"across\", \"basis\", \"benefit\", \"cash\", \"congrats\", \"congratulations\", \"continue\", \"continued\", \"continues\", \"curious\", \"delivered\", \"driving\", \"drove\", \"exceeded\", \"exceeding\", \"expansion\", \"flow\", \"generated\", \"great\", \"grew\", \"growing\", \"growth\", \"helped\", \"helping\", \"income\", \"increase\", \"increased\", \"increasing\", \"job\", \"leverage\", \"lot\", \"margin\", \"momentum\", \"nice\", \"nicely\", \"operating\", \"outperformance\", \"outstanding\", \"over\", \"performance\", \"pretty\", \"proud\", \"raising\", \"really\", \"record\", \"repurchase\", \"results\", \"share\", \"solid\", \"sustainable\", \"terrific\", \"think\", \"up\", \"upside\", \"well\", \"years\"])\n",
    "            negative_voc.extend([\"actions\", \"address\", \"affected\", \"affecting\", \"anticipated\", \"associated\", \"back\", \"believe\", \"below\", \"caused\", \"causing\", \"certain\", \"change\", \"changed\", \"changes\", \"confident\", \"costs\", \"decision\", \"decrease\", \"decreased\", \"down\", \"due\", \"dynamics\", \"expectations\", \"expected\", \"experienced\", \"factors\", \"fell\", \"goodwill\", \"happened\", \"headwinds\", \"however\", \"impact\", \"impacted\", \"impacting\", \"impacts\", \"issue\", \"issues\", \"longer\", \"lower\", \"necessary\", \"need\", \"not\", \"offset\", \"pressure\", \"pressures\", \"pronounced\", \"pushed\", \"related\", \"resolve\", \"revised\", \"short\", \"slipped\", \"soft\", \"softer\", \"softness\", \"steps\", \"taking\", \"temporary\", \"term\", \"timing\", \"transition\", \"trying\", \"understand\"])\n",
    "\n",
    "    # The below tokenizers 1) remove stop words, 2) lowercases, 3) tokenizes and 4) vectorizes (tf-idf scores for bigrams in the given vocabulary)\n",
    "    pos_vectorizer = TfidfVectorizer(stop_words=sws,ngram_range=(1,2), vocabulary=positive_voc) # Remove stopwords, and create bigrams\n",
    "    neg_vectorizer = TfidfVectorizer(stop_words=sws,ngram_range=(1,2), vocabulary=negative_voc)\n",
    "\n",
    "    # Fit and transform the training set documents\n",
    "    pos_dtm = pos_vectorizer.fit_transform(docs)\n",
    "    neg_dtm = neg_vectorizer.fit_transform(docs)\n",
    "    return pos_dtm, neg_dtm\n",
    "\n",
    "def calculate_doc_sentiment(pos_dtm_j, neg_dtm_j, Nj):\n",
    "    # Look up term frequencies for document j\n",
    "    pos_tfij = pos_dtm_j\n",
    "    neg_tfij = neg_dtm_j\n",
    "\n",
    "    # Calculate sentiment score for given document\n",
    "    pos_Sj = np.sum(pos_tfij)/Nj\n",
    "    neg_Sj = np.sum(neg_tfij)/Nj\n",
    "\n",
    "    return pos_Sj, neg_Sj\n",
    "\n",
    "def get_transcripts(filepath, transcript_column=\"transcript\"):\n",
    "    \"\"\"Returns a list consisting all transcripts at given filepath.\\n\n",
    "        Example use: Build in-sample TF-IDF scores using training_set.csv, and for each new transcript, run \n",
    "        construct_bigram_scores(list).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    transcripts = df[transcript_column].tolist()\n",
    "\n",
    "    return transcripts\n",
    "\n",
    "def construct_bigram_scores(transcripts=list, include_unigrams=False, lm=False, ghr=False):  \n",
    "    \"\"\"\n",
    "    Assumes all transcripts are sorted by date. \\n\n",
    "    transcripts is a list of transcripts. \\n\n",
    "    Builds dt tf-idf matrix for the entire dataset. \n",
    "    \"\"\"\n",
    "\n",
    "    train_sentiment_scores = [] # list of final sentiments\n",
    "\n",
    "    # Vectorize the transcripts:\n",
    "    dtm_positive, dtm_negative = vectorize_transcripts(transcripts, include_unigrams, lm, ghr)\n",
    "\n",
    "    for j in range(len(transcripts)):\n",
    "        Nj = len(transcripts[j])\n",
    "\n",
    "        score_positive, score_negative = calculate_doc_sentiment(dtm_positive[j], dtm_negative[j], Nj)\n",
    "\n",
    "        # Calculate positivity index: \n",
    "        final_sentiment = (score_positive-score_negative)/(score_positive+score_negative+1)\n",
    "\n",
    "        train_sentiment_scores.append(final_sentiment)\n",
    "        \n",
    "    return train_sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column in final_dataset with GHR-bigram TF-IDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = construct_bigram_scores(transcripts=get_transcripts(\"..\\data\\\\final_dataset.csv\")) # Should use training data to avoid lookahead here!\n",
    "\n",
    "sentiment_scores_b = sentiment_scores\n",
    "\n",
    "# Standardize sentiment scores to be between -1 and 1:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sentiment_scores = np.array(sentiment_scores)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "standardized_scores = scaler.fit_transform(sentiment_scores.reshape(-1, 1))\n",
    "\n",
    "df = pd.read_csv(\"..\\data\\\\final_dataset.csv\")\n",
    "df[\"GHR-bigram-tf-idf\"] = standardized_scores\n",
    "df.to_csv(\"..\\data\\\\final_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column in final_dataset with GHR-bigram-unigram TF-IDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = construct_bigram_scores(transcripts=get_transcripts(\"..\\data\\\\final_dataset.csv\"), include_unigrams=True, ghr=True) # Should use training data to avoid lookahead here!\n",
    "\n",
    "sentiment_scores_b = sentiment_scores\n",
    "\n",
    "# Standardize sentiment scores to be between -1 and 1:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sentiment_scores = np.array(sentiment_scores)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "standardized_scores = scaler.fit_transform(sentiment_scores.reshape(-1, 1))\n",
    "\n",
    "df = pd.read_csv(\"..\\data\\\\final_dataset.csv\")\n",
    "df[\"GHR-bigram-unigram-tf-idf\"] = standardized_scores\n",
    "df.to_csv(\"..\\data\\\\final_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column in final_dataset with GHR-bigram-LM-unigram TF-IDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = construct_bigram_scores(transcripts=get_transcripts(\"..\\data\\\\final_dataset.csv\"), include_unigrams=True, lm=True) # Should use training data to avoid lookahead here!\n",
    "\n",
    "sentiment_scores_b = sentiment_scores\n",
    "\n",
    "# Standardize sentiment scores to be between -1 and 1:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sentiment_scores = np.array(sentiment_scores)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "standardized_scores = scaler.fit_transform(sentiment_scores.reshape(-1, 1))\n",
    "\n",
    "df = pd.read_csv(\"..\\data\\\\final_dataset.csv\")\n",
    "df[\"GHR-bigram-LM-unigram-tf-idf\"] = standardized_scores\n",
    "df.to_csv(\"..\\data\\\\final_dataset.csv\", index=False) # Save as column in final_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
