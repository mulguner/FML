{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monthly:\n",
    "\n",
    "Thanks to https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920 for inspiration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = pd.read_pickle('..\\data\\portfolio_decile\\portfolio_dict_GHR-bigram-tf-idf.pkl')\n",
    "data = pd.read_csv('..\\data\\\\final_dataset.csv')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', '--', 'motley', 'ladies', 'gentlemen', 'year', \n",
    "                    'million', 'thousand', 'think', 'call', 'quarter', 'analyst', 'officer', 'like', 'month', 'rate', \n",
    "                    'one','time','u','well','would','really','first','thank','see','going','kind', 'look', 'study', 'thanks', 'also', 'last', 'operator','question','results','term',\n",
    "                    'billion','good','know','patient','third','second','get','back','lot','thing','today','right','trial', 'two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all dates to datetime\n",
    "data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "\n",
    "dates = strategy.keys()\n",
    "strategy.keys()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(transcript_string): # input: one row from dataframe\n",
    "    transcript = transcript_string.split(\". \") # could be done smarter\n",
    "    \n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    # basic lemmatization\n",
    "    def lemmatization(texts):\n",
    "        texts_out = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        for sent in texts:\n",
    "            for word in sent:\n",
    "                texts_out.append(lemmatizer.lemmatize(word))\n",
    "        return texts_out\n",
    "   \n",
    "    data_words = list(sent_to_words(transcript))\n",
    "\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    bigram = gensim.models.Phrases(data_words_nostops, min_count=5, threshold=20) # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[data_words_nostops], threshold=20)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    data_words_bigrams = [bigram_mod[transcript] for transcript in data_words_nostops]\n",
    "    data_words_trigrams = [trigram_mod[bigram_mod[transcript]] for transcript in data_words_bigrams]\n",
    "\n",
    "    data_lemmatized = lemmatization(data_words_trigrams)\n",
    "\n",
    "    data_lemmatized = [token for token in data_lemmatized if token != 'u']\n",
    "    data_lemmatized = [token for token in data_lemmatized if token != 't']\n",
    "    \n",
    "    return data_lemmatized\n",
    "\n",
    "    # return(data_lemmatized) # a list with all words and bigrams in 2decile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As interpreting these takes too long, we instead run LDA on the top/bottom 2 deciles overall, without considering the month in which each was released:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lda_corpus(transcripts): # set perf=True to see coherence, perplexity\n",
    "    \"\"\"Get LDA topics for each list of transcripts\"\"\"\n",
    "    # print(transcripts)\n",
    "    preprocessed_transcripts = [] # list of list with all words and bigrams in 2decile\n",
    "\n",
    "    for i in tqdm(transcripts.index):\n",
    "        transcript = (transcripts['transcript'][i])\n",
    "        preprocessed_transcripts.append(preprocess(transcript))      \n",
    "    \n",
    "    # Create Dictionary \n",
    "    id2word = corpora.Dictionary(preprocessed_transcripts)  \n",
    "    # Create Corpus \n",
    "    texts = preprocessed_transcripts  \n",
    "    # tdf \n",
    "    corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "    # print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])\n",
    "    return id2word, texts, corpus\n",
    "\n",
    "def train_lda(id2w, texts, corpus, perf=False):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2w,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=50,\n",
    "                                           passes=15,\n",
    "                                           alpha='symmetric',\n",
    "                                           eta=0.000000001,\n",
    "                                           per_word_topics=True) # initiation stolen from medium article\n",
    "\n",
    "    # print(lda_model.print_topics()) # run to get topics\n",
    "    if perf: # kills performance\n",
    "        perplexity_lda = lda_model.log_perplexity(corpus) # lower => better\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2w, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        return (lda_model, perplexity_lda, coherence_lda)\n",
    "\n",
    "    return lda_model, corpus, id2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_lda_pos_topics = {}\n",
    "A_lda_neg_topics = {}\n",
    "\n",
    "all_top_2deciles = []\n",
    "all_bottom_2deciles = [] \n",
    "\n",
    "for date in dates:\n",
    "    \"\"\"\n",
    "    Get LDA topics for each date for bottom and top deciles. \n",
    "    \"\"\"\n",
    "    dt_date = pd.to_datetime(date).date()\n",
    "    last_date = dt_date - pd.DateOffset(months=1)\n",
    "    # find all transcripts corresponding to given date and top/bottom 2 deciles:\n",
    "    n_stocks_in_top = strategy[date]['decile'].count(1)+strategy[date]['decile'].count(2)\n",
    "    n_stocks_in_bottom = strategy[date]['decile'].count(9)+strategy[date]['decile'].count(10)\n",
    "\n",
    "    # n_monthly_transcripts = ((len(strategy[date]['score'])))\n",
    "    # n_stocks_in_deciles = n_monthly_transcripts//5\n",
    "    top_2deciles_stocks = strategy[date]['ticker'][0:n_stocks_in_top]\n",
    "    bottom_2deciles_stocks = strategy[date]['ticker'][-n_stocks_in_bottom:]\n",
    "    \n",
    "    # find transcripts from top deciles:\n",
    "\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    dt_date = pd.to_datetime(dt_date)\n",
    "\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    top_2deciles_transcripts = data[\n",
    "        (data['date'] >= last_date) &\n",
    "        (data['date'] <= dt_date) &\n",
    "        (data['ticker'].isin(top_2deciles_stocks))\n",
    "        ]\n",
    "    \n",
    "    bottom_2deciles_transcripts = data[\n",
    "        (data['date'] >= last_date) &\n",
    "        (data['date'] <= dt_date) &\n",
    "        (data['ticker'].isin(bottom_2deciles_stocks))\n",
    "        ]\n",
    "    \n",
    "    all_top_2deciles.append(top_2deciles_transcripts)\n",
    "    all_bottom_2deciles.append(bottom_2deciles_transcripts)\n",
    "\n",
    "# Combine the dataframes\n",
    "all_top_2deciles = pd.concat(all_top_2deciles)\n",
    "all_bottom_2deciles = pd.concat(all_bottom_2deciles)\n",
    "\n",
    "# Run the LDA algorithm as above:\n",
    "pos_id2word, pos_texts, pos_corpus = generate_lda_corpus(all_top_2deciles) # (lda_model, perplexity_lda, coherence_lda)\n",
    "neg_id2word, neg_texts, neg_corpus = generate_lda_corpus(all_bottom_2deciles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_2deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lda_model, pos_corpus, pos_id2word = train_lda(pos_id2word, pos_texts, pos_corpus)\n",
    "neg_lda_model, neg_corpus, neg_id2word = train_lda(neg_id2word, neg_texts, neg_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot word clouds for the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate word clouds for each topic\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import random\n",
    "\n",
    "# def random_reds(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "#     r = random.randint(150, 255)\n",
    "    \n",
    "#     return f\"rgb({r}, 0, 0)\"  # Generate colors from dark to bright red\n",
    "\n",
    "def plot_wordclouds(lda_model, num_topics):\n",
    "    for i in range(num_topics):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Get topic terms and their weights\n",
    "        topic_terms = lda_model.show_topic(i)\n",
    "        \n",
    "        # Convert the topic terms into a dictionary suitable for WordCloud\n",
    "        wc = WordCloud(width=800, height=800, background_color='white')\n",
    "        \n",
    "        # Generate the word cloud for the i-th topic\n",
    "        wc.generate_from_frequencies(dict(topic_terms))\n",
    "        \n",
    "        # Plotting\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.title(f'Topic {i + 1}', fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "plot_wordclouds(pos_lda_model, num_topics=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of the topics, we run the following visualization (not in report):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(pos_lda_model, pos_corpus, pos_id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(neg_lda_model, neg_corpus, neg_id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
