{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "master_dictionary = pd.read_csv(\"..\\data\\Loughran-McDonald_MasterDictionary_1993-2023.csv\")\n",
    "positive_words = [word.lower() for word in master_dictionary[master_dictionary[\"Positive\"] > 0][\"Word\"].tolist()]\n",
    "negative_words = [word.lower() for word in master_dictionary[master_dictionary[\"Negative\"] > 0][\"Word\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(document, filter=False):\n",
    "    \"\"\"Returns the number of positive or negative words per document\"\"\"\n",
    "    positive_count, negative_count = 0, 0\n",
    "    \n",
    "    tokens = eval(document)\n",
    "\n",
    "    if not filter:\n",
    "        for word in tokens: \n",
    "            if word in positive_words:\n",
    "                positive_count += 1\n",
    "            elif word in negative_words:\n",
    "                negative_count += 1\n",
    "        \n",
    "    else: # Follow UZH article: Don't count positive words that include a negation in the preceeding 3 words\n",
    "        negations = [\"no\", \"not\", \"none\", \"neither\", \"never\", \"nobody\"]\n",
    "        for i in range(len(tokens)): # \n",
    "            neighbor_words = []\n",
    "\n",
    "            # add previous three words:\n",
    "            if i >= 3:\n",
    "                neighbor_words.extend([tokens[i-1], tokens[i-2], tokens[i-3]])\n",
    "\n",
    "            # Don't count positive words if negation:\n",
    "            neighbor_flag = False\n",
    "            for negation in negations:\n",
    "                if negation in neighbor_words: neighbor_flag = True\n",
    "                if \"n't\" in negation: neighbor_flag = True # Flag if e.g. shouldn't preceedes a positive word\n",
    "                \n",
    "            if tokens[i] in positive_words and not neighbor_flag:\n",
    "                positive_count += 1\n",
    "            elif tokens[i] in negative_words:\n",
    "                negative_count += 1\n",
    "\n",
    "            # Could extend this to consider other cases, e.g. screening for \"special\" n-grams like good \"morning\"\n",
    "            # as proposed by https://www.zora.uzh.ch/id/eprint/199785/1/SSRN-id2559157.pdf p. 8. \n",
    "            # The method above will not lead to considerable differences, but serves as an illustrative \n",
    "            # example of how to improve on the naive BoW approach without breaking its framework.\n",
    "            # We will use this (enhanced) BoW approach going forward, considering this as part of the preprocessing step.\n",
    "\n",
    "    return positive_count, negative_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BoW_score(document):\n",
    "    \"\"\"Returns sentiment score for each document\"\"\"\n",
    "    positive_count, negative_count = get_count(document)\n",
    "    positivity_score = (positive_count-negative_count)/(negative_count+positive_count+1)\n",
    "    return positivity_score\n",
    "\n",
    "def Improved_BoW_score(document):\n",
    "    \"\"\"Returns negativity score for each document controlling for negations in prior three tokens\"\"\"\n",
    "    \"\"\"We will use this one for inferences.\"\"\"\n",
    "    positive_count, negative_count = get_count(document, filter=True)\n",
    "    positivity_score = (positive_count-negative_count)/(negative_count+positive_count+1)\n",
    "    return positivity_score\n",
    "\n",
    "def run_BoW(data):\n",
    "    \"\"\"Get BoW sentiment score for each document in cleaned data\"\"\"\n",
    "    df = pd.read_csv(data)\n",
    "\n",
    "    # naive_negativity_scores = []\n",
    "    enhanced_positivity_scores = []\n",
    "    ticker = []\n",
    "    date = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        document = df[\"tokenized_transcript\"][i]\n",
    "    \n",
    "        # naive_negativity_scores.append(get_BoW_score(document))\n",
    "        enhanced_positivity_scores.append(Improved_BoW_score(document))\n",
    "\n",
    "        ticker.append(df[\"ticker\"][i])\n",
    "        date.append(df[\"Origin_Date\"][i])\n",
    "\n",
    "        if i%100 == 0: \n",
    "            print(f\"Percentage done: {round(i*100/len(df), 4)}%.\")\n",
    "\n",
    "    return enhanced_positivity_scores, ticker, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_positivity_scores, ticker, date = run_BoW(\"..\\data\\BoW-tokenized-transcripts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the preprocessed transcripts rather than the raw data, we achieve a speedup of around 90% in the encoding.\n",
    "\n",
    "# Standardize sentiment scores to be between -1 and 1:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "sentiment_scores = np.array(enhanced_positivity_scores)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "standardized_scores = scaler.fit_transform(sentiment_scores.reshape(-1, 1))\n",
    "\n",
    "lst = standardized_scores.tolist()\n",
    "standardized_scores = [score[0] for score in lst]\n",
    "\n",
    "bow_scores = pd.DataFrame(list(zip(ticker, date, standardized_scores)), columns=[\"ticker\", \"Origin_Date\", \"enhanced-bow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in final_dataset:\n",
    "df = pd.read_csv(\"..\\data\\\\final_dataset.csv\")\n",
    "new_df = pd.merge(df, bow_scores,  how='left', left_on=['date_time','ticker'], right_on = ['Origin_Date','ticker'])\n",
    "new_df.drop(columns=['Origin_Date'], axis=1, inplace=True)\n",
    "# dropna, should have 15803 rows or so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"..\\data\\\\final_dataset.csv\", index=False) # Save final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final_dataset without transcripts\n",
    "fds = pd.read_csv(\"..\\data\\\\final_dataset.csv\")\n",
    "fds.drop(columns=[\"transcript\"], inplace=True)\n",
    "fds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds.to_csv(\"..\\data\\\\no_transcript_final_dataset.csv\", index=False) # Save no_transcript_final_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
